{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a language model's performance on a dataset \n",
    "\n",
    "I have tried two methods and they both works. Feel free to try and compare two methods' efficiency.\n",
    "\n",
    "1. **First Method (Straightforward Approach)**: \n",
    "   - This method utilizes a defined function (`get_answer`) that takes structured input and generates predictions from the language model. It is straightforward and easy to understand, making it ideal for users who prefer a clear and direct approach to model evaluation. The function encapsulates the logic for processing input data and invoking the model, ensuring that the evaluation process is organized and maintainable.\n",
    "\n",
    "2. **Second Method (Creative Approach)**: \n",
    "   - The second method employs a lambda function as the `llm_or_chain_factory`. This approach allows for more creativity in how predictions are generated and evaluated. While it can lead to more concise code, it requires careful handling of input and output structures. This method is suitable for users who want to experiment with different ways of processing input data and generating model outputs, providing flexibility in the evaluation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Set your API key env\n",
    "os.environ['GROQ_API_KEY'] = \"your groq key\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"your langchain key\"  # Update with your API key\n",
    "\n",
    " \n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-groq-8b-8192-tool-use-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import pandas as pd\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.smith import RunEvalConfig\n",
    "import uuid\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = \"Spar.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file_path).head(5)\n",
    "\n",
    "# Initialize the LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Upload the dataset\n",
    "try:\n",
    "    dataset = client.upload_csv(\n",
    "        csv_file=csv_file_path,\n",
    "        input_keys=[\"Story\", \"Question\", \"Candidate_Answers\"],  # Adjust based on your CSV structure\n",
    "        output_keys=[\"Answer\"],\n",
    "        name=f\"spar_human_{str(uuid.uuid4())}\"\n",
    "    )\n",
    "    print(\"Dataset uploaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading dataset: {e}\")\n",
    "\n",
    "# Define the model\n",
    "model = ChatGroq(\n",
    "    model=\"llama3-groq-8b-8192-tool-use-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Modify get_answer to accept input as a parameter\n",
    "def get_answer(input_):\n",
    "    story = input_[\"Story\"]\n",
    "    question = input_[\"Question\"]\n",
    "    options = input_[\"Candidate_Answers\"].split(', ')  # Assuming options are comma-separated\n",
    "\n",
    "    # Create messages for the current model\n",
    "    messages = [\n",
    "        SystemMessage(content=Visual_prompt),  # Ensure Visual_prompt is defined\n",
    "        HumanMessage(content=f\"Given the context: {story} and question: {question}\\nOptions: {options}\\nPlease select the correct answer.\")\n",
    "    ]\n",
    "    \n",
    "    # Invoke the model and return the output\n",
    "    return model.invoke(messages).content\n",
    "\n",
    "# Define evaluation configuration\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"exact_match\"]  # Use the \"exact_match\" evaluator\n",
    ")\n",
    "\n",
    "# Run the evaluation using the LangSmith client and the dataset\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset.name,  # The data to predict and grade over\n",
    "    llm_or_chain_factory=get_answer,  # Provide a function to generate the predicted output\n",
    "    evaluation=eval_config,  # The evaluators to score the results\n",
    "    project_metadata={\"version\": \"1.0.0\", \"model\": model.model_name},  # The name of the experiment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langsmith import Client\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain.smith import RunEvalConfig\n",
    "import uuid\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = \"Spar.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Initialize the LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Upload the dataset\n",
    "try:\n",
    "    dataset = client.upload_csv(\n",
    "        csv_file=csv_file_path,\n",
    "        input_keys=[\"Story\", \"Question\", \"Candidate_Answers\"],  # Adjust based on your CSV structure\n",
    "        output_keys=[\"Answer\"],\n",
    "        name=f\"spar_human_{str(uuid.uuid4())}\"\n",
    "    )\n",
    "    print(\"Dataset uploaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading dataset: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model):\n",
    "    results = []  \n",
    "    story = input_[\"Story\"]\n",
    "    question = input_[\"Question\"]\n",
    "    options = input_[\"Candidate_Answers\"].split(', ') \n",
    "    ground_truth = output_[\"Answer\"]  # Assuming options are comma-separated\n",
    "\n",
    "    # Create messages for the current model\n",
    "    messages = [\n",
    "        SystemMessage(content=Visual_prompt),  # Ensure Visual_prompt is defined\n",
    "        HumanMessage(content=f\"Given the context: {story} and question: {question}\\nOptions: {options}\\nPlease select the correct answer.\")\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Generate output from the LLM using the messages\n",
    "        output = model.invoke(messages)\n",
    "\n",
    "        # Use the runnable evaluator\n",
    "        eval_config = RunEvalConfig(\n",
    "            evaluators=[\"exact_match\"],  # Use the \"exact_match\" evaluator\n",
    "            prediction_key=\"predicted\",  # Key for the model's output\n",
    "            reference_key=\"Answer\"  # Use the correct key for ground truth\n",
    "        )\n",
    "        \n",
    "        # Run the evaluation using the LangSmith client and the dataset\n",
    "        client.run_on_dataset(\n",
    "            dataset_name=dataset.name,  # The data to predict and grade over\n",
    "            llm_or_chain_factory=lambda input_: {\"predicted\": output.content, \"Answer\": ground_truth},  # Provide a function to generate the predicted output\n",
    "            evaluation=eval_config,  # The evaluators to score the results\n",
    "            project_metadata={\"version\": \"1.0.0\", \"model\": model.model_name},  # The name of the experiment\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"model\": model.model_name,\n",
    "            \"question\": question,\n",
    "            \"predicted\": output.content,\n",
    "            \"ground_truth\": ground_truth,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking model {model.model_name}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama3-groq-8b-8192-tool-use-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "test= evaluate_model(model)\n",
    "test.to_csv(\"test_result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
